{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukaszlewickii/spark_labs/blob/main/df2_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIFY-fkHstqV"
      },
      "source": [
        "# DataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsDNcGDMstqX"
      },
      "source": [
        "http://spark.apache.org/docs/2.2.0/api/python/pyspark.sql.html#pyspark-sql-module"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIuckPH6tFf6",
        "outputId": "dba65205-8672-4eb4-d4db-ee84f68853db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=e6f79620ccc660b402b53d87e02b1e3e765af043fc412da9e783a18bb65a32e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoJtnQVbstqY"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "import pyspark.sql.functions as f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edbLpCihstqY"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName('DataFrame_2') \\\n",
        "    .master('local[*]') \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "cMHXsHEZstqZ",
        "outputId": "2ee07b24-9054-4013-c18a-115845e00649"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-205ac4692d3b>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#https://github.com/apache/spark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.../'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpeople\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'people.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0memployees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'employees.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpeople_txt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'people.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/people.json."
          ]
        }
      ],
      "source": [
        "#https://github.com/apache/spark\n",
        "data_path = '.../'\n",
        "people = spark.read.json('people.json')\n",
        "employees = spark.read.json('employees.json')\n",
        "people_txt = spark.read.option(\"inferSchema\", \"true\").csv('people.txt')\n",
        "people_txt = people_txt.withColumnRenamed('_c0', 'name').withColumnRenamed('_c1', 'age')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "74iVse22stqZ"
      },
      "outputs": [],
      "source": [
        "people.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "IN1thm_8stqZ"
      },
      "outputs": [],
      "source": [
        "employees.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "_gTvdioVstqZ"
      },
      "outputs": [],
      "source": [
        "people_txt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5F5nMuRstqZ"
      },
      "source": [
        "#### join\n",
        "inner (domyslny)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "FsNMtt26stqZ"
      },
      "outputs": [],
      "source": [
        "people.join(other=employees, on='name', how='inner').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS2MYyB8stqZ"
      },
      "source": [
        "Uwaga ogolna\n",
        "Join to stosunkowo popularna, ale kosztowna operacja.\n",
        "W sytuacji, kiedy jeden z laczonych DataFramow jest znacznie mniejszy (w szczegolnosci na tyle maly, ze w calosci miesci sie w pamieci), zaleca sie zastosowanie broadcast hash join.\n",
        "(Mala tabela zostanie zebrana do pamieci i wyslana do kazdego noda).\n",
        "W niektorych przypadkach optymalizator sam za nas zdecyduje o zastosowaniu broadcast hash join. Jednak SparkSQL wyjatkowo tutaj daje nam mozliwosc wymuszenia tej operacji wprost w kodzie.\n",
        "from pyspark.sql.functions import broadcast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RijNgc9Gstqa"
      },
      "source": [
        "> **TODO**: Znajdź ile zarabia najmlodsza osoba spośród (people, people_txt)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKTUnoRtstqa"
      },
      "source": [
        "> **TODO**: Dla kazdego pracownika (employees), dla ktorego mamy informacje o wieku (people, people_txt) dodaj do pensji 0.1% za kazdy rok zycia. Zsumuj koszt takiego 'bonusu urodzinowego' dla pracodawcy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og0KW7Ewstqa"
      },
      "source": [
        "#### groupBy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGik5JJYstqa"
      },
      "outputs": [],
      "source": [
        "people.groupBy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZymOKtlstqa"
      },
      "source": [
        " Przez GroupedData mamy dostep do takich funkcji jak:<br>\n",
        " avg, max, min, sum, count, agg <br>\n",
        " (dla wygody, do funkcji 'agg' mamy tez dostep bezposrednio na DataFrame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "56ktUe7fstqa"
      },
      "outputs": [],
      "source": [
        "people.groupBy().max('age').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CmkFHAjTstqa"
      },
      "outputs": [],
      "source": [
        "people.groupBy('name').count().show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1XWnPDQstqb"
      },
      "outputs": [],
      "source": [
        "#from pyspark.sql import functions as f\n",
        "people.groupBy('name').agg(f.min('age').alias('min_age'), f.max('age').alias('max_age'), f.count('name').alias('n_people')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "SjQYqZapstqb"
      },
      "outputs": [],
      "source": [
        "people.agg(f.min('age'), f.max('age'), f.count('name')).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8aND5xustqb"
      },
      "source": [
        "> **TODO**: Ile jest unikatowych (wystepujacych tylko 1 raz) imion w polaczonych zbiorach people oraz people_txt? (allPeople)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgyErFL2stqb"
      },
      "source": [
        "> **TODO**: Ile lat maja osoby, ktorych imiona wystepuja tylko raz w polaczonych zbiorach people oraz people_txt?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG7v9xZfstqb"
      },
      "source": [
        "****"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEQ2oKsHstqb"
      },
      "outputs": [],
      "source": [
        "# Wygenerujmy nowy DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awR4qPh8stqb"
      },
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZPN9lHEstqb"
      },
      "outputs": [],
      "source": [
        "years = 10\n",
        "names = ['Alice', 'Betty', 'Chris', 'Dan', 'Greg']\n",
        "unique_names_count = len(names)\n",
        "names = sorted(names*years)\n",
        "year = [y for y in range(2000, 2000+years)]*len(names)\n",
        "starting_salary = [round(random.gauss(4000, 1000),2) for i in range(unique_names_count)]\n",
        "salary = [0 for i in range(years*unique_names_count)]\n",
        "salary[::years] = starting_salary\n",
        "for n in range(unique_names_count):\n",
        "    for y in range(years-1):\n",
        "        index = (years*n+1)+y\n",
        "        #print(index, salary[index-1])\n",
        "        salary[index] = round(salary[index-1]*(1+random.gauss(0.1,0.09)),2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMp8CM1dstqb"
      },
      "outputs": [],
      "source": [
        "salaryHistory = spark.createDataFrame([Row(name=n, year=y, salary=s) for n,y,s in zip(names, year, salary)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWtHH5kistqc"
      },
      "outputs": [],
      "source": [
        "salaryHistory = salaryHistory.filter((salaryHistory['name'] != 'Greg') | (salaryHistory['year'] != 2006))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARFtF61Pstqc"
      },
      "outputs": [],
      "source": [
        "salaryHistory = salaryHistory.union(spark.createDataFrame([Row('Alice', 3000, 2000)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "I_JM9gTSstqc"
      },
      "outputs": [],
      "source": [
        "salaryHistory.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AlbRXSBstqc"
      },
      "source": [
        "> **TODO**: Przyjrzyj sie nowemu zbiorowi danych salaryHistory.<br>\n",
        "a. Zobacz schemat. <br>\n",
        "b. Ile rekordow jest w calym zbiorze? <br>\n",
        "c. Jaka jest najmniejsza i najwieksza pensja?<br>\n",
        "d. Ile razy powtarza sie kazde z imion?<br>\n",
        "e. Stworz tabele sredniej, minimalnej i maksymalnej pensji w zależności od roku. Posortuj lata malejaco. Pensje podaj z dokladnoscia do pelnych wartosci. <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQgRSgvFstqc"
      },
      "source": [
        "#### Window functions\n",
        "over"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpz9PAgsstqc"
      },
      "source": [
        "Do obliczania agregowanych wartosci w grupach definiowanych oknem (window).<br>\n",
        "Zwracaja wiele rekordow (tyle ile na wejsciu w grupie)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K32qqAcHstqc"
      },
      "source": [
        "partitionBy - definiuje podzial danych na okna<br>\n",
        "orderBy - definiuje sortowanie wewnatrz kazdego z okien<br>\n",
        "Frame (rangeBetween/rowsBetween) - definiuje offset<br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7BApdE-stqc"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.window import Window\n",
        "# from pyspark.sql import functions as f"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfSJ2WYsstqc"
      },
      "source": [
        "partitionBy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n2naGK8stqc"
      },
      "outputs": [],
      "source": [
        "# definicja 'okna'\n",
        "myWindowSpec = Window.partitionBy(allPeople['name'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acjJjg1rstqd"
      },
      "source": [
        "Funkcje agregujące okien (aka funkcje okien lub agregacje okienkowe ) <br>\n",
        "Funkcje okna działają w odniesieniu do grupy wierszy, nazywanej oknem, i obliczania wartości zwracanej dla każdego wiersza w oparciu o grupę wierszy. Funkcje okna są przydatne do przetwarzania zadań, takich jak Obliczanie średniej przenoszonej, obliczanie zbiorczej statystyki lub uzyskiwanie dostępu do wartości wierszy, w których podano względne położenie bieżącego wiersza."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "jBUS7NtPstqd"
      },
      "outputs": [],
      "source": [
        "# wywolanie funkcji na kazdym 'oknie'\n",
        "allPeople.withColumn('nameCount', f.count(allPeople['name']).over(myWindowSpec)).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAHLm-nustqd"
      },
      "source": [
        "> **TODO**: Do zbioru salaryHistory dodaj kolumne 'avgSalaryDiff', ktora bedzie zawierala roznice pomiedzy pensja z danego roku, a srednia pensja osoby na przestrzeni wszytskich lat."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKFIRT3Istqd"
      },
      "source": [
        "partitionBy + orderBy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-ebwoo40stqd"
      },
      "outputs": [],
      "source": [
        "# np. rank\n",
        "# - musimy zdefiniowac dodatkowo sortowanie wewnatrz kazdej z grup\n",
        "# - zwraca lp dla kolejnych rekordow posortowanych wedlug zadanych kolumn\n",
        "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year'])\n",
        "ranked = (f.rank()).over(windowSpec)\n",
        "salaryHistory.withColumn('ranked', ranked).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0V-uSW9_stqd"
      },
      "outputs": [],
      "source": [
        "salaryHistory_tmp = salaryHistory.filter(salaryHistory.name.isin('Alice', 'Greg'))\n",
        "salaryHistory_tmp.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptI8AJu-stqd"
      },
      "source": [
        "> **TODO**: Dla zbioru salaryHistory, porownaj pensje ludzi pomiedzy najwczesniejszym a najpozniejszym rokiem ich pracy.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn42tctVstqe"
      },
      "source": [
        "partitionBy + orderBy + rangeBetween/rowsBetween"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOSRQokNstqe"
      },
      "outputs": [],
      "source": [
        "# np. srednia ruchoma\n",
        "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year']).rangeBetween(-1,0)\n",
        "movingAvg = (f.avg(salaryHistory['salary'])).over(windowSpec)\n",
        "salaryHistory.withColumn('movingAvg', movingAvg).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_2qHLgpstqe"
      },
      "outputs": [],
      "source": [
        "# np. srednia ze wszystkich rekordow do aktualnego wlacznie\n",
        "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year']).rowsBetween(Window.unboundedPreceding,Window.currentRow)\n",
        "movingAvg = (f.avg(salaryHistory['salary'])).over(windowSpec)\n",
        "salaryHistory.withColumn('movingAvg', movingAvg).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "K6gPso2fstqe"
      },
      "outputs": [],
      "source": [
        "# podobny efekt uzyskamy ponizszym zapytaniam. Drobna roznica: rekordy w jednej grupie (imie, rok) nie zostana rozdzielone\n",
        "import sys\n",
        "windowSpec = Window.partitionBy(salaryHistory['name']).orderBy(salaryHistory['year']).rangeBetween(-sys.maxsize,0)\n",
        "movingAvg = (f.avg(salaryHistory['salary'])).over(windowSpec)\n",
        "salaryHistory.withColumn('movingAvg', movingAvg).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJC19Kpbstqe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}